{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector embeddings Loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.300d.txt' \n",
    "# (glove data set from: https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "#filename = 'numberbatch-en.txt'\n",
    "#(https://github.com/commonsense/conceptnet-numberbatch)\n",
    "\n",
    "def loadembeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Word vector embeddings Loaded.')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "# Pre-trained word embedding\n",
    "vocab,embd = loadembeddings(filename)\n",
    "\n",
    "word_vec_dim = len(embd[0]) # word_vec_dim = dimension of each word vectors\n",
    "\n",
    "#e = np.zeros((word_vec_dim,),np.float32)+0.0001\n",
    "\n",
    "vocab.append('<UNK>') #<UNK> represents unknown word\n",
    "embdunk = np.random.randn(word_vec_dim) #np.asarray(embd[vocab.index('unk')],np.float32)+e\n",
    "    \n",
    "vocab.append('<EOS>') #<EOS> represents end of sentence\n",
    "embdeos = np.random.randn(word_vec_dim) #np.asarray(embd[vocab.index('eos')],np.float32)+e\n",
    "\n",
    "vocab.append('<PAD>') #<PAD> represents paddings\n",
    "\n",
    "\n",
    "embd.append(embdunk)  \n",
    "embd.append(embdeos)  \n",
    "    \n",
    "embdpad = np.zeros(word_vec_dim)\n",
    "embd.append(embdpad)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP TEN MOST SIMILAR WORDS TO 'frog':\n",
      "\n",
      "1. frog\n",
      "2. toad\n",
      "3. frogs\n",
      "4. monkey\n",
      "5. squirrel\n",
      "6. snake\n",
      "7. toads\n",
      "8. rodent\n",
      "9. 65stk\n",
      "10. parrot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jishnu/anaconda3/envs/deeplearn/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    }
   ],
   "source": [
    "def most_similar_cosine(x):\n",
    "    #embed = embedding[0:len(embedding)-1]\n",
    "    embed = embedding\n",
    "    xdoty = np.multiply(embed,x) #element-wise\n",
    "    xdoty = np.sum(xdoty,1)\n",
    "    xlen = np.square(x)\n",
    "    xlen = np.sum(xlen,0)\n",
    "    xlen = np.sqrt(xlen)\n",
    "    ylen = np.square(embed)\n",
    "    ylen = np.sum(ylen,1)\n",
    "    ylen = np.sqrt(ylen)\n",
    "    xlenylen = np.multiply(xlen,ylen)\n",
    "    cosine_similarities = np.divide(xdoty,xlenylen)\n",
    "    return np.flip(np.argsort(cosine_similarities),0)\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "word = 'frog'\n",
    "\n",
    "most_similars = most_similar_eucli(word2vec(word))\n",
    "\n",
    "print(\"TOP TEN MOST SIMILAR WORDS TO '\"+str(word)+\"':\\n\")\n",
    "for i in range(0,10):\n",
    "    print(str(i+1)+\". \"+str(vocab[most_similars[i]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data 0\n",
      "Processing data 10000\n",
      "Processing data 20000\n",
      "Processing data 30000\n",
      "Processing data 40000\n",
      "Processing data 50000\n",
      "Processing data 60000\n",
      "Processing data 70000\n",
      "Processing data 80000\n",
      "Processing data 90000\n",
      "Processing data 100000\n",
      "Processing data 110000\n",
      "Processing data 120000\n",
      "Processing data 130000\n",
      "Processing data 140000\n",
      "Processing data 150000\n",
      "Processing data 160000\n",
      "Processing data 170000\n",
      "Processing data 180000\n",
      "Processing data 190000\n",
      "Processing data 200000\n",
      "Processing data 210000\n",
      "Processing data 220000\n",
      "Processing data 230000\n",
      "Processing data 240000\n",
      "Processing data 250000\n",
      "Processing data 260000\n",
      "Processing data 270000\n",
      "Processing data 280000\n",
      "Processing data 290000\n",
      "Processing data 300000\n",
      "Processing data 310000\n",
      "Processing data 320000\n",
      "Processing data 330000\n",
      "Processing data 340000\n",
      "Processing data 350000\n",
      "Processing data 360000\n",
      "Processing data 370000\n",
      "Processing data 380000\n",
      "Processing data 390000\n",
      "Processing data 400000\n",
      "Processing data 410000\n",
      "Processing data 420000\n",
      "Processing data 430000\n",
      "Processing data 440000\n",
      "Processing data 450000\n",
      "Processing data 460000\n",
      "Processing data 470000\n",
      "Processing data 480000\n",
      "Processing data 490000\n",
      "Processing data 500000\n",
      "Processing data 510000\n",
      "Processing data 520000\n",
      "Processing data 530000\n",
      "Processing data 540000\n",
      "Processing data 550000\n",
      "Processing data 560000\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "summaries = []\n",
    "texts = []\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    printable = set(string.printable)\n",
    "    text = \"\".join(list(filter(lambda x: x in printable, text))) #filter funny characters, if any.\n",
    "    #print(text)\n",
    "    text = text.translate(str.maketrans('', '',string.punctuation))\n",
    "    #print(text)\n",
    "    return text\n",
    "\n",
    "#max_data = 100000\n",
    "i=0\n",
    "with open('Reviews.csv', 'rt') as csvfile: #Data from https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "    Reviews = csv.DictReader(csvfile)\n",
    "    count=0\n",
    "    for row in Reviews:\n",
    "        #if count<max_data:\n",
    "        clean_text = clean(row['Text'])\n",
    "        clean_summary = clean(row['Summary'])\n",
    "        summaries.append(word_tokenize(clean_summary))\n",
    "        texts.append(word_tokenize(clean_text))\n",
    "        #count+=1\n",
    "        if i%10000==0:\n",
    "            print(\"Processing data {}\".format(i))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size of data: 279480\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "texts_v2 = []\n",
    "summaries_v2 = []\n",
    "\n",
    "max_len_text = 80\n",
    "max_len_sum = 4\n",
    "for text in texts:\n",
    "    if(len(text)<=max_len_text and len(summaries[i])<=max_len_sum): \n",
    "        #remove data pairs with review length more than max_len_text\n",
    "        #or summary length more than max_len_sum\n",
    "        texts_v2.append(text)\n",
    "        summaries_v2.append(summaries[i])\n",
    "    i+=1\n",
    "    \n",
    "print(\"Current size of data: \"+str(len(texts_v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {word:i for i,word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size of data: 255540\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "texts = []\n",
    "summaries = []\n",
    "\n",
    "for summary in summaries_v2:\n",
    "    flag = 0    \n",
    "    for word in summary:\n",
    "        if word not in vocab_dict:\n",
    "            flag = 1\n",
    "            \n",
    "    #Remove summary and its corresponding text \n",
    "    #if out of vocabulary word present in summary\n",
    "    \n",
    "    if flag == 0:\n",
    "        summaries.append(summary)\n",
    "        texts.append(texts_v2[i])\n",
    "    i+=1\n",
    "\n",
    "print(\"Current size of data: \"+str(len(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#REDUCE DATA (FOR SPEEDING UP THE NEXT STEPS)\\n\\nMAXIMUM_DATA_NUM = 20000\\n\\ntexts = texts_v3[0:MAXIMUM_DATA_NUM]\\nsummaries = summaries_v3[0:MAXIMUM_DATA_NUM]\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#REDUCE DATA (FOR SPEEDING UP THE NEXT STEPS)\n",
    "\n",
    "MAXIMUM_DATA_NUM = 20000\n",
    "\n",
    "texts = texts_v3[0:MAXIMUM_DATA_NUM]\n",
    "summaries = summaries_v3[0:MAXIMUM_DATA_NUM]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHUFFLE\n",
    "\n",
    "import random\n",
    "\n",
    "texts_idx = [idx for idx in range(0,len(texts))]\n",
    "random.shuffle(texts_idx)\n",
    "\n",
    "texts = [texts[idx] for idx in texts_idx]\n",
    "summaries = [summaries[idx] for idx in texts_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE CLEANED & TOKENIZED TEXT: \n",
      "\n",
      "['these', 'nut', 'bars', 'are', 'perfect', 'if', 'you', 'want', 'a', 'big', 'filling', 'snack', 'they', 'are', 'much', 'more', 'like', 'candy', 'than', 'i', 'was', 'expecting', 'much', 'like', 'a', 'payday', 'bar', 'but', 'with', 'more', 'sweet', 'stuff', 'on', 'the', 'outside', 'not', 'the', 'best', 'for', 'watching', 'the', 'waistline', 'but', 'very', 'yummy']\n",
      "\n",
      "SAMPLE CLEANED & TOKENIZED SUMMARY: \n",
      "\n",
      "['filling', 'snack']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "index = random.randint(0,len(texts)-1)\n",
    "\n",
    "print(\"SAMPLE CLEANED & TOKENIZED TEXT: \\n\\n\"+str(texts[index]))\n",
    "print(\"\\nSAMPLE CLEANED & TOKENIZED SUMMARY: \\n\\n\"+str(summaries[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(.7*len(texts))\n",
    "val_len = int(.2*len(texts))\n",
    "\n",
    "train_summaries = summaries[0:train_len]\n",
    "train_texts = texts[0:train_len]\n",
    "\n",
    "val_summaries = summaries[train_len:val_len+train_len]\n",
    "val_texts = texts[train_len:train_len+val_len]\n",
    "\n",
    "test_summaries = summaries[train_len+val_len:]\n",
    "test_texts = texts[train_len+val_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_and_batch(texts,summaries):\n",
    "    lentexts = []\n",
    "\n",
    "    i=0\n",
    "    for text in texts:\n",
    "        lentexts.append(len(text))\n",
    "        i+=1\n",
    "        \n",
    "    #print(len(texts))\n",
    "    #print(len(summaries))\n",
    "\n",
    "    sortedindex = np.flip(np.argsort(lentexts),axis=0)\n",
    "    #sort indexes according to the sequence length of corresponding texts. \n",
    "    \n",
    "    batch_size = 50\n",
    "\n",
    "    bi=0\n",
    "\n",
    "    batches_x = []\n",
    "    batches_y = []\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "\n",
    "    for i in range(0,len(texts)):\n",
    "\n",
    "        if bi>=batch_size:\n",
    "            bi=0\n",
    "            #print(batch_x)\n",
    "            #print(batch_y)\n",
    "            batches_x.append(batch_x)\n",
    "            batches_y.append(batch_y)\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            \n",
    "        if bi==0:\n",
    "            max_len = len(texts[int(sortedindex[i])])\n",
    "        \n",
    "        text = []\n",
    "        summary = []\n",
    "        \n",
    "        for j in range(0,max_len):\n",
    "            if j==len(texts[int(sortedindex[i])]):\n",
    "                text.append(vocab_dict['<EOS>'])\n",
    "            elif j > len(texts[int(sortedindex[i])]):\n",
    "                text.append(vocab_dict['<PAD>'])\n",
    "            else:\n",
    "                text.append(vocab_dict.get(texts[int(sortedindex[i])][j],vocab_dict['<UNK>']))\n",
    "                \n",
    "        for j in range(0,5):\n",
    "            if j==len(summaries[int(sortedindex[i])]):\n",
    "                summary.append(vocab_dict['<EOS>'])\n",
    "            elif j > len(summaries[int(sortedindex[i])]):\n",
    "                summary.append(vocab_dict['<PAD>'])\n",
    "            else:\n",
    "                summary.append(vocab_dict.get(summaries[int(sortedindex[i])][j],vocab_dict['<UNK>']))\n",
    "                \n",
    "\n",
    "        batch_x.append(text)\n",
    "        batch_y.append(summary)\n",
    "\n",
    "        bi+=1\n",
    "        \n",
    "    return batches_x,batches_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_x,train_batches_y = bucket_and_batch(train_texts,train_summaries)\n",
    "val_batches_x,val_batches_y = bucket_and_batch(val_texts,val_summaries)\n",
    "test_batches_x,test_batches_y = bucket_and_batch(test_texts,test_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving processed data in another file.\n",
    "\n",
    "import pickle\n",
    "\n",
    "PICK = [vocab,embd,50,train_batches_x,train_batches_y,val_batches_x,val_batches_y,test_batches_x,test_batches_y]\n",
    "\n",
    "with open('AmazonPICKLE', 'wb') as fp:\n",
    "    pickle.dump(PICK, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

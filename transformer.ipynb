{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector embeddings Loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.50d.txt' \n",
    "# (glove data set from: https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "#filename = 'numberbatch-en.txt'\n",
    "#(https://github.com/commonsense/conceptnet-numberbatch)\n",
    "\n",
    "def loadembeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Word vector embeddings Loaded.')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "# Pre-trained word embedding\n",
    "vocab,embd = loadembeddings(filename)\n",
    "\n",
    "word_vec_dim = len(embd[0]) # word_vec_dim = dimension of each word vectors\n",
    "\n",
    "e = np.zeros((word_vec_dim,),np.float32)+0.0001\n",
    "\n",
    "vocab.append('<UNK>') #<UNK> represents unknown word\n",
    "embdunk = np.asarray(embd[vocab.index('unk')],np.float32)+e\n",
    "    \n",
    "vocab.append('<EOS>') #<EOS> represents end of sentence\n",
    "embdeos = np.asarray(embd[vocab.index('eos')],np.float32)+e\n",
    "\n",
    "vocab.append('<PAD>') #<PAD> represents paddings\n",
    "\n",
    "flag1=0\n",
    "flag2=0\n",
    "\n",
    "for vec in embd:\n",
    "    \n",
    "    if np.all(np.equal(np.asarray(vec,np.float32),embdunk)):\n",
    "        flag1=1\n",
    "        print \"FLAG1\"   \n",
    "    if np.all(np.equal(np.asarray(vec,np.float32),embdeos)):\n",
    "        flag2=1\n",
    "        print \"FLAG2\"\n",
    "\n",
    "if flag1==0:\n",
    "    embd.append(embdunk)  \n",
    "if flag2 == 0:\n",
    "    embd.append(embdeos)  \n",
    "    \n",
    "embdpad = np.zeros(word_vec_dim)\n",
    "embd.append(embdpad)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open ('AmazonPICKLE', 'rb') as fp:\n",
    "    PICK = pickle.load(fp)\n",
    "\n",
    "vocab_limit = PICK[0]\n",
    "vocab_len = len(vocab_limit)\n",
    "\n",
    "batch_size = int(PICK[1])\n",
    "\n",
    "batches_x = PICK[2]\n",
    "batches_y = PICK[3]\n",
    "\n",
    "batches_x_pe = PICK[4] #already position encoded\n",
    "\n",
    "max_len = len(batches_y[0][0]) #max output len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len,model_dimensions):\n",
    "    pe = np.zeros((seq_len,model_dimensions,),np.float32)\n",
    "    for pos in xrange(0,seq_len):\n",
    "        for i in xrange(0,model_dimensions):\n",
    "            pe[pos][i] = math.sin(pos/(10000**(2*i/model_dimensions)))\n",
    "    return pe.reshape((seq_len,model_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare training data\n",
    "\n",
    "train_len = int(0.75*len(batches_x))\n",
    "\n",
    "train_batches_x= batches_x[0:train_len]\n",
    "train_batches_x_pe = batches_x_pe[0:train_len]\n",
    "\n",
    "train_batches_y = batches_y[0:train_len]\n",
    "\n",
    "# (Rest of the data can be used for validating and testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "h=5\n",
    "N=4 #no. of decoder and encoder layers\n",
    "learning_rate=0.003\n",
    "iters = 5\n",
    "x = tf.placeholder(tf.float32, [batch_size,None,word_vec_dim])\n",
    "y = tf.placeholder(tf.int32, [batch_size,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#modified version of def LN used here: \n",
    "#https://theneuralperspective.com/2016/10/27/gradient-topics/\n",
    "\n",
    "def layer_norm(inputs, epsilon = 1e-5):\n",
    "\n",
    "    mean, var = tf.nn.moments(inputs, [1,2], keep_dims=True)\n",
    "    \n",
    "    scale = tf.Variable(tf.ones([1,1,word_vec_dim]),\n",
    "                        dtype=tf.float32)\n",
    "    shift = tf.Variable(tf.ones([1,1,word_vec_dim]),\n",
    "                        dtype=tf.float32)\n",
    "    LN = scale * (inputs - mean) / tf.sqrt(var + epsilon) + shift\n",
    " \n",
    "    return LN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def attention(Q,K,V,d):\n",
    "\n",
    "    K_ = tf.transpose(K,[0,2,1])\n",
    " \n",
    "    d = tf.cast(d,tf.float32)\n",
    "    result = tf.matmul(tf.nn.softmax(tf.div(tf.matmul(Q,K_),tf.sqrt(d))),V)\n",
    " \n",
    "    return result\n",
    "\n",
    "def masked_attention(Q,K,V,d,pos):\n",
    "    \n",
    "    K_ = tf.transpose(K,[0,2,1])\n",
    "    d = tf.cast(d,tf.float32)\n",
    "    \n",
    "    softmax_component = tf.div(tf.matmul(Q,K_),tf.sqrt(d))\n",
    "    \n",
    "    softmax_component = tf.reshape(softmax_component,[batch_size,max_len,max_len])\n",
    "    \n",
    "    mask = np.zeros((max_len,max_len),dtype=np.float32)\n",
    "    mask[pos:max_len,:] = -2**30\n",
    "    mask[:,pos:max_len] = -2**30\n",
    "    mask = tf.convert_to_tensor(mask)\n",
    "    mask = tf.reshape(mask,[1,max_len,max_len])\n",
    "    \n",
    "    softmax_component = softmax_component + mask\n",
    "    \n",
    "    result = tf.matmul(tf.nn.softmax(softmax_component),V)\n",
    "    \n",
    "    return result\n",
    "       \n",
    "\n",
    "def multihead_attention(Q,K,V,pos=0,mask=False):\n",
    "    \n",
    "    Q_ = tf.reshape(Q,[-1,tf.shape(Q)[2]])\n",
    "    K_ = tf.reshape(K,[-1,tf.shape(Q)[2]])\n",
    "    V_ = tf.reshape(V,[-1,tf.shape(Q)[2]])\n",
    "    \n",
    "    d = 30\n",
    "    \n",
    "    heads = tf.TensorArray(size=h,dtype=tf.float32)\n",
    "    \n",
    "    Wq = tf.Variable(tf.truncated_normal(shape=[h,word_vec_dim,d],stddev=0.01))\n",
    "    Wk = tf.Variable(tf.truncated_normal(shape=[h,word_vec_dim,d],stddev=0.01))\n",
    "    Wv = tf.Variable(tf.truncated_normal(shape=[h,word_vec_dim,d],stddev=0.01))\n",
    "    Wo = tf.Variable(tf.truncated_normal(shape=[h*d,word_vec_dim],stddev=0.01))\n",
    "    \n",
    "    for i in xrange(0,h):\n",
    "        \n",
    "        Q_w = tf.matmul(Q_,Wq[i])\n",
    "        Q_w = tf.reshape(Q_w,[tf.shape(Q)[0],tf.shape(Q)[1],d])\n",
    "        \n",
    "        K_w = tf.matmul(K_,Wk[i])\n",
    "        K_w = tf.reshape(K_w,[tf.shape(K)[0],tf.shape(K)[1],d])\n",
    "        \n",
    "        V_w = tf.matmul(V_,Wv[i])\n",
    "        V_w = tf.reshape(V_w,[tf.shape(V)[0],tf.shape(V)[1],d])\n",
    "        \n",
    "        if mask == False:\n",
    "            head = attention(Q_w,K_w,V_w,d)\n",
    "        else:\n",
    "            head = masked_attention(Q_w,K_w,V_w,d,pos)\n",
    "            \n",
    "        heads = heads.write(i,head)\n",
    "        \n",
    "    heads = heads.stack()\n",
    "    \n",
    "    #heads shape hxbatchxseqxd\n",
    "    \n",
    "    concated = heads[0]\n",
    "    \n",
    "    for i in xrange(1,h):\n",
    "        concated = tf.concat([concated,heads[i]],2)\n",
    "\n",
    "    concated = tf.reshape(concated,[-1,h*d])\n",
    "    out = tf.matmul(concated,Wo)\n",
    "    out = tf.reshape(out,[tf.shape(heads)[1],tf.shape(heads)[2],word_vec_dim])\n",
    "    \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    \n",
    "    d = 1024\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,d],stddev=0.01))\n",
    "    b1 = tf.Variable(tf.truncated_normal(shape=[1,d],stddev=0.01))\n",
    "    W2 = tf.Variable(tf.truncated_normal(shape=[d,word_vec_dim],stddev=0.01))\n",
    "    b2 = tf.Variable(tf.truncated_normal(shape=[1,word_vec_dim],stddev=0.01))\n",
    "    \n",
    "    sublayer1 = multihead_attention(x,x,x)\n",
    "    sublayer1 = layer_norm(sublayer1 + x)\n",
    "    \n",
    "    sublayer1_ = tf.reshape(sublayer1,[-1,word_vec_dim])\n",
    "    \n",
    "    sublayer2 = tf.matmul(tf.nn.relu(tf.matmul(sublayer1_,W1)+b1),W2) + b2\n",
    "    sublayer2 = tf.reshape(sublayer2,tf.shape(x))\n",
    "    \n",
    "    sublayer2 = layer_norm(sublayer2 + sublayer1)\n",
    "    \n",
    "    return sublayer2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(y,enc_out,mask=False,pos=0):\n",
    "    \n",
    "    d=1024\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,d],stddev=0.01))\n",
    "    b1 = tf.Variable(tf.truncated_normal(shape=[1,d],stddev=0.01))\n",
    "    W2 = tf.Variable(tf.truncated_normal(shape=[d,word_vec_dim],stddev=0.01))\n",
    "    b2 = tf.Variable(tf.truncated_normal(shape=[1,word_vec_dim],stddev=0.01))\n",
    "\n",
    "    sublayer1 = multihead_attention(y,y,y,pos,mask)\n",
    "    sublayer1 = layer_norm(sublayer1 + y)\n",
    "    \n",
    "    sublayer2 = multihead_attention(sublayer1,enc_out,enc_out)\n",
    "    sublayer2 = layer_norm(sublayer2 + sublayer1)\n",
    "    \n",
    "    sublayer2_ = tf.reshape(sublayer2,[-1,word_vec_dim])\n",
    "    \n",
    "    sublayer3 = tf.matmul(tf.nn.relu(tf.matmul(sublayer2_,W1)+b1),W2) + b2\n",
    "    sublayer3 = tf.reshape(sublayer3,tf.shape(y))\n",
    "    sublayer3 = layer_norm(sublayer3 + sublayer2)\n",
    "    \n",
    "    return sublayer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    \n",
    "    encoderin = x\n",
    "    pe = tf.constant(positional_encoding(max_len,word_vec_dim)) #all position encodings\n",
    "    pe = tf.reshape(pe,[max_len,1,word_vec_dim])\n",
    "    #encoder layers\n",
    "    for i in xrange(0,N):\n",
    "        encoderin = encoder(encoderin)\n",
    "    encoderout = encoderin\n",
    "    \n",
    "    #decoder_layers\n",
    "    \n",
    "    decoderin_part1 = tf.ones([batch_size,1,word_vec_dim],dtype=tf.float32)\n",
    "    filled = 1 #no. of output words that are filled\n",
    "    \n",
    "    #initial decoder values for all known output words \n",
    "    #(currently nothing is known, except the fixed value at the left head of the output\n",
    "    # which is being initialized and which will be right-shifted later)\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal(shape=[max_len*word_vec_dim,word_vec_dim],stddev=0.01))\n",
    "    B = tf.Variable(tf.truncated_normal(shape=[1,word_vec_dim],stddev=0.01))\n",
    "    \n",
    "    for i in xrange(0,max_len):\n",
    "        \n",
    "        decoderin_part2 = tf.constant(np.full((batch_size,(max_len-filled),word_vec_dim),0,dtype=np.float32))\n",
    "        decoderin = tf.concat([decoderin_part1,decoderin_part2],1)\n",
    "        \n",
    "        for j in xrange(0,N):\n",
    "            decoderin = decoder(decoderin,encoderout,mask=True,pos=filled)\n",
    "            \n",
    "        decoderout = decoderin\n",
    "        \n",
    "        # decoderout = batch_size x max_len x word_vec_dim\n",
    "        # we want it to be = batch_size x word_vec_dim\n",
    "        # which can be linearly transformed to represent the probability distributions.\n",
    "        \n",
    "        #decoderout = tf.reshape(decoderout,[batch_size,max_len*word_vec_dim])\n",
    "        # now: decoder_concat = batch_size x (max_len*word_vec_dim)\n",
    "    \n",
    "        #decoderout = tf.nn.relu(tf.matmul(decoderout,W) + B)\n",
    "        # now decoderout should be batch_size x word_vec_dim\n",
    "        \n",
    "        decoderout = decoderout[:,i]\n",
    "        #print decoderout\n",
    "        \n",
    "        decoderout = decoderout + pe[i]\n",
    "        # now decoderout is position encoded.\n",
    "        \n",
    "        decoderout = tf.reshape(decoderout,[batch_size,1,word_vec_dim])\n",
    "        \n",
    "        if i==0:\n",
    "            decoderin_part1 = decoderout\n",
    "            filled = 1 \n",
    "        else:\n",
    "            decoderin_part1 = tf.concat([decoderin_part1,decoderout],1)\n",
    "            filled += 1\n",
    "        \n",
    "        decoderin = decoderout\n",
    "    \n",
    "        \n",
    "    Wl = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,vocab_len],stddev=0.01))\n",
    "    bl = tf.Variable(tf.truncated_normal(shape=[1,vocab_len],stddev=0.01))\n",
    "      \n",
    "    decoderout = decoderin_part1\n",
    "\n",
    "    decoderout_ = tf.reshape(decoderout,[-1,word_vec_dim])\n",
    "        \n",
    "    out_probs = tf.nn.relu(tf.matmul(decoderout_,Wl)+bl)\n",
    "    out_probs = tf.reshape(out_probs,[batch_size,max_len,vocab_len])\n",
    "    \n",
    "    return out_probs          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(x)\n",
    "\n",
    "#OPTIMIZER\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=0.9,beta2=0.98,epsilon=1e-9).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_out(output_batch):\n",
    "    out = []\n",
    "    for output_text in output_batch:\n",
    "        output_len = len(output_text)\n",
    "        transformed_output = np.zeros([output_len],dtype=np.int32)\n",
    "        for i in xrange(0,output_len):\n",
    "            transformed_output[i] = vocab_limit.index(vec2word(output_text[i]))\n",
    "        out.append(transformed_output)\n",
    "    return np.asarray(out,np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHOSEN SAMPLE NO.: 18\n",
      "\n",
      "Epoch: 1 Iteration: 1\n",
      "\n",
      "SAMPLE TEXT:\n",
      "the coffee shown is great tasting and quick for those on the go <PAD> \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "dropper gravely gravely gravely gravely\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great tasting <EOS> <PAD> <PAD> \n",
      "\n",
      "loss=9.63818\n",
      "\n",
      "CHOSEN SAMPLE NO.: 0\n",
      "\n",
      "Epoch: 1 Iteration: 2\n",
      "\n",
      "SAMPLE TEXT:\n",
      "i love these licorice get licorice <UNK> just sugar with some flavor very satisfying \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "sweet tasted <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great licorice flavor <EOS> <PAD> \n",
      "\n",
      "loss=9.45367\n",
      "\n",
      "CHOSEN SAMPLE NO.: 6\n",
      "\n",
      "Epoch: 1 Iteration: 3\n",
      "\n",
      "SAMPLE TEXT:\n",
      "low <UNK> self hand <UNK> no best before <UNK> no nutrition <UNK> no ingredients <PAD> \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great in <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "bad bad bad <EOS> <PAD> \n",
      "\n",
      "loss=9.33323\n",
      "\n",
      "CHOSEN SAMPLE NO.: 45\n",
      "\n",
      "Epoch: 1 Iteration: 4\n",
      "\n",
      "SAMPLE TEXT:\n",
      "tried these on the recommendation from <UNK> tall pale and handsome grandson they are unbelievable \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great price <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great cookies <EOS> <PAD> <PAD> \n",
      "\n",
      "loss=9.21648\n",
      "\n",
      "CHOSEN SAMPLE NO.: 39\n",
      "\n",
      "Epoch: 1 Iteration: 5\n",
      "\n",
      "SAMPLE TEXT:\n",
      "product actually arrived sooner than i expected and was properly packaged and in good condition \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "good price <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "prompt delivery <EOS> <PAD> <PAD> \n",
      "\n",
      "loss=9.10827\n",
      "\n",
      "CHOSEN SAMPLE NO.: 0\n",
      "\n",
      "Epoch: 1 Iteration: 6\n",
      "\n",
      "SAMPLE TEXT:\n",
      "i have always liked oberto sausage and these short bite sized links are very handy <PAD> \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great price <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "oberto sausage sticks <EOS> <PAD> \n",
      "\n",
      "loss=8.95913\n",
      "\n",
      "CHOSEN SAMPLE NO.: 41\n",
      "\n",
      "Epoch: 1 Iteration: 7\n",
      "\n",
      "SAMPLE TEXT:\n",
      "these rice are the best i have already ordered them twice and will continue doing so \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great price <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "best <EOS> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=8.77209\n",
      "\n",
      "CHOSEN SAMPLE NO.: 28\n",
      "\n",
      "Epoch: 1 Iteration: 8\n",
      "\n",
      "SAMPLE TEXT:\n",
      "good but not as good as the australian licorice that actually have strawberry seeds in it \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great price <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "strawberry licorice <EOS> <PAD> <PAD> \n",
      "\n",
      "loss=8.5904\n",
      "\n",
      "CHOSEN SAMPLE NO.: 25\n",
      "\n",
      "Epoch: 1 Iteration: 9\n",
      "\n",
      "SAMPLE TEXT:\n",
      "<UNK> products are great i would recommend them for anyone who wants a wheat free product \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "good price <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "super product <EOS> <PAD> <PAD> \n",
      "\n",
      "loss=8.6737\n",
      "\n",
      "CHOSEN SAMPLE NO.: 9\n",
      "\n",
      "Epoch: 1 Iteration: 10\n",
      "\n",
      "SAMPLE TEXT:\n",
      "price went up too much i think i will go to sams club from now on <PAD> \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great good <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "high price <EOS> <PAD> <PAD> \n",
      "\n",
      "loss=8.40646\n",
      "\n",
      "CHOSEN SAMPLE NO.: 22\n",
      "\n",
      "Epoch: 1 Iteration: 11\n",
      "\n",
      "SAMPLE TEXT:\n",
      "this is the second time that i ordered this product and ive been very happy with it \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great good <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "very good vanilla <EOS> <PAD> \n",
      "\n",
      "loss=8.30349\n",
      "\n",
      "CHOSEN SAMPLE NO.: 13\n",
      "\n",
      "Epoch: 1 Iteration: 12\n",
      "\n",
      "SAMPLE TEXT:\n",
      "this stuff is crap it kills flowers there leaves curdles after a week waste of time money \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great good <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "crap <EOS> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=8.20646\n",
      "\n",
      "CHOSEN SAMPLE NO.: 22\n",
      "\n",
      "Epoch: 1 Iteration: 13\n",
      "\n",
      "SAMPLE TEXT:\n",
      "we like the whole grain brown rice and this is very good the package sizing is convenient <PAD> \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great good <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "nice rice <EOS> <PAD> <PAD> \n",
      "\n",
      "loss=8.1188\n",
      "\n",
      "CHOSEN SAMPLE NO.: 15\n",
      "\n",
      "Epoch: 1 Iteration: 14\n",
      "\n",
      "SAMPLE TEXT:\n",
      "product arrived in great <UNK> easy given that it was 12 cans of pumpkin arrived fast very pleased \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "good good <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "organic pumpkin <EOS> <PAD> <PAD> \n",
      "\n",
      "loss=8.00883\n",
      "\n",
      "CHOSEN SAMPLE NO.: 43\n",
      "\n",
      "Epoch: 1 Iteration: 15\n",
      "\n",
      "SAMPLE TEXT:\n",
      "this stuff is powerful be careful when you first use it not for lightweights but does the job \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "good good <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "packs a punch <EOS> <PAD> \n",
      "\n",
      "loss=7.71536\n",
      "\n",
      "CHOSEN SAMPLE NO.: 31\n",
      "\n",
      "Epoch: 1 Iteration: 16\n",
      "\n",
      "SAMPLE TEXT:\n",
      "this yogurt is thick and creamy and tastes delicious mixed with your favorite lowfat or <UNK> breakfast cereal <PAD> \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great good <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "delicious <EOS> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=7.59537\n",
      "\n",
      "CHOSEN SAMPLE NO.: 40\n",
      "\n",
      "Epoch: 1 Iteration: 17\n",
      "\n",
      "SAMPLE TEXT:\n",
      "these chips are so good feel like im eating real potato chips what flavor i love all of them \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great <EOS> <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "these are wonderful <EOS> <PAD> \n",
      "\n",
      "loss=7.30603\n",
      "\n",
      "CHOSEN SAMPLE NO.: 34\n",
      "\n",
      "Epoch: 1 Iteration: 18\n",
      "\n",
      "SAMPLE TEXT:\n",
      "this is amazing mixed with plain greek style yogurt i cant wait to try some of the other flavors \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great <EOS> <PAD> <PAD> <PAD>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "amazing <EOS> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=7.41412\n",
      "\n",
      "CHOSEN SAMPLE NO.: 2\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from __future__ import print_function\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 0   \n",
    "    best_loss = 999\n",
    "    display_step = 1\n",
    "    \n",
    "    while step < iters:\n",
    "           \n",
    "        batch_len = len(train_batches_x_pe)\n",
    "        for i in xrange(0,batch_len):\n",
    "            \n",
    "            sample_no = np.random.randint(0,batch_size)\n",
    "            print(\"\\nCHOSEN SAMPLE NO.: \"+str(sample_no))\n",
    "            \n",
    "            train_out = transform_out(train_batches_y[i])\n",
    "            \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nEpoch: \"+str(step+1)+\" Iteration: \"+str(i+1))\n",
    "                print(\"\\nSAMPLE TEXT:\")\n",
    "                for vec in train_batches_x[i][sample_no]:\n",
    "                    print(str(vec2word(vec)),end=\" \")\n",
    "                print(\"\\n\")\n",
    "\n",
    "\n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,out = sess.run([optimizer,cost,output],feed_dict={x: train_batches_x_pe[i], \n",
    "                                                                     y: train_out})\n",
    "            \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nPREDICTED SUMMARY OF THE SAMPLE:\\n\")\n",
    "                flag = 0\n",
    "                for array in out[sample_no]:\n",
    "                    #if int(index)!=vocab_limit.index('eos'):\n",
    "                    if vocab_limit[np.argmax(array)] in string.punctuation or flag==0:\n",
    "                        print(str(vocab_limit[np.argmax(array)]),end='')\n",
    "                    else:\n",
    "                        print(\" \"+str(vocab_limit[np.argmax(array)]),end='')\n",
    "                    flag=1\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                print(\"ACTUAL SUMMARY OF THE SAMPLE:\\n\")\n",
    "                for vec in batches_y[i][sample_no]:\n",
    "                    print(str(vec2word(vec)),end=\" \")\n",
    "                print(\"\\n\")\n",
    "            \n",
    "            print(\"loss=\"+str(loss))\n",
    "                  \n",
    "            if(loss<best_loss):\n",
    "                best_loss = loss\n",
    "                saver.save(sess, 'Model_Backup/allattmodel.ckpt')\n",
    "\n",
    "        step=step+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

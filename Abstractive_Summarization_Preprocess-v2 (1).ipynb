{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector embeddings Loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.50d.txt' \n",
    "# (glove data set from: https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "#filename = 'numberbatch-en.txt'\n",
    "#(https://github.com/commonsense/conceptnet-numberbatch)\n",
    "\n",
    "def loadembeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Word vector embeddings Loaded.')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "# Pre-trained word embedding\n",
    "vocab,embd = loadembeddings(filename)\n",
    "\n",
    "word_vec_dim = len(embd[0]) # word_vec_dim = dimension of each word vectors\n",
    "\n",
    "e = np.zeros((word_vec_dim,),np.float32)+0.0001\n",
    "\n",
    "vocab.append('<UNK>') #<UNK> represents unknown word\n",
    "embdunk = np.asarray(embd[vocab.index('unk')],np.float32)+e\n",
    "    \n",
    "vocab.append('<EOS>') #<EOS> represents end of sentence\n",
    "embdeos = np.asarray(embd[vocab.index('eos')],np.float32)+e\n",
    "\n",
    "vocab.append('<PAD>') #<PAD> represents paddings\n",
    "\n",
    "flag1=0\n",
    "flag2=0\n",
    "\n",
    "for vec in embd:\n",
    "    \n",
    "    if np.all(np.equal(np.asarray(vec,np.float32),embdunk)):\n",
    "        flag1=1\n",
    "        print \"FLAG1\"   \n",
    "    if np.all(np.equal(np.asarray(vec,np.float32),embdeos)):\n",
    "        flag2=1\n",
    "        print \"FLAG2\"\n",
    "\n",
    "if flag1==0:\n",
    "    embd.append(embdunk)  \n",
    "if flag2 == 0:\n",
    "    embd.append(embdeos)  \n",
    "    \n",
    "embdpad = np.zeros(word_vec_dim)\n",
    "embd.append(embdpad)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP TEN MOST SIMILAR WORDS TO 'frog':\n",
      "\n",
      "1. frog\n",
      "2. snake\n",
      "3. ape\n",
      "4. toad\n",
      "5. monkey\n",
      "6. spider\n",
      "7. lizard\n",
      "8. tarantula\n",
      "9. cat\n",
      "10. spiny\n"
     ]
    }
   ],
   "source": [
    "def most_similar_cosine(x):\n",
    "    #embed = embedding[0:len(embedding)-1]\n",
    "    embed = embedding\n",
    "    xdoty = np.multiply(embed,x) #element-wise\n",
    "    xdoty = np.sum(xdoty,1)\n",
    "    xlen = np.square(x)\n",
    "    xlen = np.sum(xlen,0)\n",
    "    xlen = np.sqrt(xlen)\n",
    "    ylen = np.square(embed)\n",
    "    ylen = np.sum(ylen,1)\n",
    "    ylen = np.sqrt(ylen)\n",
    "    xlenylen = np.multiply(xlen,ylen)\n",
    "    cosine_similarities = np.divide(xdoty,xlenylen)\n",
    "    return np.flip(np.argsort(cosine_similarities),0)\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "word = 'frog'\n",
    "\n",
    "most_similars = most_similar_eucli(word2vec(word))\n",
    "\n",
    "print \"TOP TEN MOST SIMILAR WORDS TO '\"+str(word)+\"':\\n\"\n",
    "for i in xrange(0,10):\n",
    "    print str(i+1)+\". \"+str(vocab[most_similars[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "summaries = []\n",
    "texts = []\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    printable = set(string.printable)\n",
    "    text = filter(lambda x: x in printable, text) #filter funny characters, if any.\n",
    "    text = text.translate(None, string.punctuation)\n",
    "    return text\n",
    "\n",
    "max_data = 100000\n",
    "with open('Reviews.csv', 'rb') as csvfile: #Data from https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "    Reviews = csv.DictReader(csvfile)\n",
    "    count=0\n",
    "    for row in Reviews:\n",
    "        if count<max_data:\n",
    "            clean_text = clean(row['Text'])\n",
    "            clean_summary = clean(row['Summary'])\n",
    "            summaries.append(word_tokenize(clean_summary))\n",
    "            texts.append(word_tokenize(clean_text))\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size of data: 48478\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "texts_v2 = []\n",
    "summaries_v2 = []\n",
    "\n",
    "max_len_text = 80\n",
    "max_len_sum = 4\n",
    "for text in texts:\n",
    "    if(len(text)<=max_len_text and len(summaries[i])<=max_len_sum): \n",
    "        #remove data pairs with review length more than max_len_text\n",
    "        #or summary length more than max_len_sum\n",
    "        texts_v2.append(text)\n",
    "        summaries_v2.append(summaries[i])\n",
    "    i+=1\n",
    "    \n",
    "print \"Current size of data: \"+str(len(texts_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size of data: 44413\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "texts_v3 = []\n",
    "summaries_v3 = []\n",
    "\n",
    "for summary in summaries_v2:\n",
    "    flag = 0    \n",
    "    for word in summary:\n",
    "        if word not in vocab:\n",
    "            flag = 1\n",
    "            \n",
    "    #Remove summary and its corresponding text \n",
    "    #if out of vocabulary word present in summary\n",
    "    \n",
    "    if flag == 0:\n",
    "        summaries_v3.append(summary)\n",
    "        texts_v3.append(texts_v2[i])\n",
    "    i+=1\n",
    "\n",
    "print \"Current size of data: \"+str(len(texts_v3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#REDUCE DATA (FOR SPEEDING UP THE NEXT STEPS)\n",
    "\n",
    "MAXIMUM_DATA_NUM = 20000\n",
    "\n",
    "texts = texts_v3[0:MAXIMUM_DATA_NUM]\n",
    "summaries = summaries_v3[0:MAXIMUM_DATA_NUM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE CLEANED & TOKENIZED TEXT: \n",
      "\n",
      "['our', 'boston', 'terrier', 'loves', 'these', 'bones', 'we', 'give', 'them', 'to', 'her', 'as', 'a', 'treat', 'or', 'to', 'keep', 'her', 'busy', 'when', 'we', 'have', 'company', 'for', 'a', '16', 'lbs', 'dog', 'shes', 'a', 'mighty', 'chewer', 'and', 'these', 'last', 'her', 'a', 'couple', 'of', 'hours', 'with', 'breaks', 'to', 'investigate', 'if', 'shes', 'missing', 'anything', 'well', 'buy', 'more', 'of', 'these']\n",
      "\n",
      "SAMPLE CLEANED & TOKENIZED SUMMARY: \n",
      "\n",
      "['chloe', 'loves', 'them']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "index = random.randint(0,len(texts)-1)\n",
    "\n",
    "print \"SAMPLE CLEANED & TOKENIZED TEXT: \\n\\n\"+str(texts[index])\n",
    "print \"\\nSAMPLE CLEANED & TOKENIZED SUMMARY: \\n\\n\"+str(summaries[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_limit = []\n",
    "i=0\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        if word not in vocab_limit:\n",
    "            if word in vocab:\n",
    "                vocab_limit.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for summary in summaries:\n",
    "    for word in summary:\n",
    "        if word not in vocab_limit:\n",
    "            if word in vocab:\n",
    "                vocab_limit.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_limit.append('<EOS>')\n",
    "vocab_limit.append('<UNK>')\n",
    "vocab_limit.append('<PAD>') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lentexts = []\n",
    "\n",
    "i=0\n",
    "for text in texts:\n",
    "    lentexts.append(len(text))\n",
    "    i+=1\n",
    "    \n",
    "sortedindex = np.argsort(lentexts)\n",
    "#sort indexes according to the sequence length of corresponding texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "bi=0\n",
    "\n",
    "batches_x = []\n",
    "batches_y = []\n",
    "batch_x = []\n",
    "batch_y = []\n",
    "\n",
    "for i in xrange(0,len(texts)):\n",
    "    \n",
    "    if bi>=batch_size:\n",
    "        bi=0\n",
    "        batches_x.append(batch_x)\n",
    "        batches_y.append(batch_y)\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "    batch_x.append(texts[int(sortedindex[i])])\n",
    "    batch_y.append(summaries[int(sortedindex[i])])\n",
    "    \n",
    "    bi+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "vec_batches_x = []\n",
    "vec_batches_x_pe = []\n",
    "\n",
    "for batch in batches_x:\n",
    " \n",
    "    max_len_x = len(batch[batch_size-1])\n",
    "    vec_texts = []\n",
    "    vec_texts_pe = []\n",
    "    \n",
    "    for text in batch:\n",
    "        \n",
    "        vec_text=[]\n",
    "        vec_text_pe = []\n",
    "    \n",
    "        pos=0\n",
    "        \n",
    "        for word in text:\n",
    "            \n",
    "            pe = np.zeros((word_vec_dim,),np.float32)\n",
    "            #positional encoding\n",
    "            \n",
    "            for i in xrange(0,word_vec_dim):\n",
    "                pe[i] = math.sin(pos/math.pow(10000,(2*i/word_vec_dim)))\n",
    "            \n",
    "            vec_text.append(word2vec(word))\n",
    "            \n",
    "            ped = np.asarray(word2vec(word),np.float32) + pe\n",
    "            \n",
    "            vec_text_pe.append(ped)\n",
    "            \n",
    "            pos=pos+1\n",
    "        \n",
    "        n = len(vec_text)\n",
    "        \n",
    "        while n<max_len_x:\n",
    "            \n",
    "            vec_text.append(word2vec('<PAD>'))\n",
    "            vec_text_pe.append(word2vec('<PAD>'))\n",
    "            n = len(vec_text)\n",
    "        \n",
    "        vec_texts.append(vec_text)\n",
    "        vec_texts_pe.append(vec_text_pe)\n",
    "    \n",
    "    vec_texts = np.asarray(vec_texts,np.float32)\n",
    "    vec_batches_x.append(vec_texts)\n",
    "    \n",
    "    vec_texts_pe = np.asarray(vec_texts_pe,np.float32)\n",
    "    vec_batches_x_pe.append(vec_texts_pe)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_batches_y = []\n",
    "#vec_batches_y_pe = []\n",
    "\n",
    "#k=0\n",
    "for batch in batches_y:\n",
    "\n",
    "    max_len_y = max_len_sum+1\n",
    "    vec_summaries = []\n",
    "\n",
    "    for summary in batch:\n",
    "        \n",
    "        vec_summary=[]\n",
    "        for word in summary:\n",
    "            vec_summary.append(word2vec(word))\n",
    "        \n",
    "        vec_summary.append(word2vec('<EOS>'))\n",
    "        \n",
    "        n = len(vec_summary)\n",
    "\n",
    "        while n<max_len_y:\n",
    "            vec_summary.append(word2vec('<PAD>'))\n",
    "            n = len(vec_summary)\n",
    "        #print n\n",
    "        \n",
    "        vec_summaries.append(vec_summary)\n",
    "    \n",
    "    vec_summaries = np.asarray(vec_summaries,np.float32)\n",
    "    vec_batches_y.append(vec_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving processed data in another file.\n",
    "\n",
    "import pickle\n",
    "\n",
    "PICK = [vocab_limit,batch_size,vec_batches_x,vec_batches_y,vec_batches_x_pe,vec]\n",
    "\n",
    "with open('AmazonPICKLE', 'wb') as fp:\n",
    "    pickle.dump(PICK, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
